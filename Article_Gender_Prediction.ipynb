{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Gender from the First Name with an LSTM using Keras\n",
    "In this short article, I want to go over how we can use a very basic LSTM to predict the gender from a given first name. Since there are many great courses on the math and general concepts behind Recurring Neural Networks (RNN), e.g. [Andrew Ng's deep learning specialization](https://www.coursera.org/specializations/deep-learning) or on [Medium](https://medium.com/search?q=lstm), I will not dig deeper into them and perceive this knowledge as given. Instead, we will only focus on the high-level implementation using Keras. The goal is to get a more practical understanding of decisions one has to make building a neural network like this, especially on how to chose some of the hyper parameters.\n",
    "\n",
    "On Keras: Latest since its TensorFlow Support in 2017, [Keras](https://keras.io/) has made a huge splash a relatively easy to use and intuitive interface into more complex machine learning libraries. As you will see, building the actual neural network, as well as training the model is going to be the shortest part in our script.\n",
    "\n",
    "The first step is to determine the type of network we want to use since that decision can impact our data preparation process. In a name, the order of characters matters, meaning that, if we want to analyze a name using a neural network, RNN are the logical choice. Long Short-Term Memory (LSTM) as a special form of RNNs are especially powerful when it comes to finding the right features when the chain of input-chunks becomes longer. Input in our case is always a string (the name) and the output a 1x2 vector indicating if the name belongs to a male or a female person.\n",
    "\n",
    "After making this decision, We will start with loading all the packages that we will need as well as the dataset - a file containing over 1.5 Mio German users with their name and gender (encoded as <i>f</i> for female and <i>m</i> for male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "filepath = 'd:/AS_Data/temp/name_test.csv'\n",
    "max_rows = 500000 # Reduction due to memory limitations\n",
    "\n",
    "df = (pd.read_csv(filepath, usecols=['first_name', 'gender'])\n",
    "        .dropna(subset=['first_name', 'gender'])\n",
    "        .assign(first_name = lambda x: x.first_name.str.strip())\n",
    "        .head(max_rows))\n",
    "\n",
    "# In the case of a middle name, we will simply use the first name only\n",
    "df['first_firstname'] = df['first_name'].apply(lambda x: str(x).split(' ', 1)[0])\n",
    "\n",
    "# Sometimes people only but the first letter of their name into the field, so we drop all name where len <3\n",
    "df.drop(df[df['first_firstname'].str.len() < 3].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "The next step in any natural language processing is to convert the input into a machine-readable vector format. In theory, neural networks in Keras are able to handle inputs with a variable shape. In praxis, however, having a fixed input length in Keras can improve performance noticeably, especially during the training. The reason for this behavior is that fixed lengths allow for the creation of tensors of fixed shaped and therefore more stable weights.\n",
    "\n",
    "First, we will convert every (first) name into a vector. The method we'll be using is the so-called One-Hot Encoding. \n",
    "Here, every word is represented by a vector of n binary sub-vectors, where n is the number of different chars in the alphabet (26 using the English alphabet). The reason why we can not simply convert every character to its position in the alphabet, e.g. a - 1, b - 2 etc.) is that this would lead the network to assume that the characters are on an ordinal scale, instead of a categorical - z not is \"worth more\" than a.\n",
    "\n",
    "Example:<br>\n",
    "s becomes:<br>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "_hello_ becomes:<br>\n",
    "[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br>\n",
    "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br>\n",
    " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br>\n",
    " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br>\n",
    " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "\n",
    "Now that we determined how the input has to look like, we have two decisions to make: How long shall the char vector be (how many different chars do we allow for) and how long shall the name vector be (how many chars we want to look at). We will only allow for the most common characters in the German alphabet (standard latin + öäü) and the hyphen, which is part of many older names.\n",
    "For simplicity purposes, we will set the length of the name vector to be the length of the longest name in our dataset, but with 25 as an upper bound to make sure our input vector doesn't grow too large just because one person made a mistake during the name entering the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input vector will have the shape 23x30.\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "predictor_col = 'first_firstname'\n",
    "result_col = 'gender'\n",
    "\n",
    "accepted_chars = 'abcdefghijklmnopqrstuvwxyzöäü-'\n",
    "\n",
    "word_vec_length = min(df[predictor_col].apply(len).max(), 25) # Length of the input vector\n",
    "char_vec_length = len(accepted_chars) # Length of the character vector\n",
    "output_labels = 2 # Number of output labels\n",
    "\n",
    "print(f\"The input vector will have the shape {word_vec_length}x{char_vec_length}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn already incorporates a One Hot Encoding algorithm in it's preprocessing library. However, in this case, because of our special situation that we are not converting labels into vectors but split every string apart into its characters, the creation of a custom algorithm seemed to be quicker than the preprocessing otherwise needed.\n",
    "\n",
    "[It has been shown](https://www.draketo.de/english/python-memory-numpy-list-array) that Numpy arrays need around 4 times less memory compared to Python lists. For that reason, we use list comprehension as a more pythonic way of creating the input array but already convert every word vector into an array inside of the list. When working with Numpy arrays, we have to make sure that all lists and/or arrays that are getting combined have the same shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(accepted_chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(accepted_chars))\n",
    "\n",
    "# Removes all non accepted characters\n",
    "def normalize(line):\n",
    "    return [c.lower() for c in line if c.lower() in accepted_chars]\n",
    "\n",
    "# Returns a list of n lists with n = word_vec_length\n",
    "def name_encoding(name):\n",
    "\n",
    "    # Encode input data to int, e.g. a->1, z->26\n",
    "    integer_encoded = [char_to_int[char] for i, char in enumerate(name) if i < word_vec_length]\n",
    "    \n",
    "    # Start one-hot-encoding\n",
    "    onehot_encoded = list()\n",
    "    \n",
    "    for value in integer_encoded:\n",
    "        # create a list of n zeros, where n is equal to the number of accepted characters\n",
    "        letter = [0 for _ in range(char_vec_length)]\n",
    "        letter[value] = 1\n",
    "        onehot_encoded.append(letter)\n",
    "        \n",
    "    # Fill up list to the max length. Lists need do have equal length to be able to convert it into an array\n",
    "    for _ in range(word_vec_length - len(name)):\n",
    "        onehot_encoded.append([0 for _ in range(char_vec_length)])\n",
    "        \n",
    "    return onehot_encoded\n",
    "\n",
    "# Encode the output labels\n",
    "def lable_encoding(gender_series):\n",
    "    labels = np.empty((0, 2))\n",
    "    for i in gender_series:\n",
    "        if i == 'm':\n",
    "            labels = np.append(labels, [[1,0]], axis=0)\n",
    "        else:\n",
    "            labels = np.append(labels, [[0,1]], axis=0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset in 60% train, 20% test and 20% validation\n",
    "train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "\n",
    "# Convert both the input names as well as the output lables into the discussed machine readable vector format\n",
    "train_x =  np.asarray([np.asarray(name_encoding(normalize(name))) for name in train[predictor_col]])\n",
    "train_y = lable_encoding(train.gender)\n",
    "\n",
    "validate_x = np.asarray([name_encoding(normalize(name)) for name in validate[predictor_col]])\n",
    "validate_y = lable_encoding(validate.gender)\n",
    "\n",
    "test_x = np.asarray([name_encoding(normalize(name)) for name in test[predictor_col]])\n",
    "test_y = lable_encoding(test.gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our input ready, we can start building our neural network. We already decided on the model (LSTM). In Keras we can simply stack multiple layers on top of each other, for this we need to initialize the model as _Sequential()_.\n",
    "\n",
    "## Choosing the right amount of nodes and layers\n",
    "\n",
    "There is no final, definite, rule of thumb on how many nodes (or hidden neurons) or how many layers you should choose, and very often a trial and error approach will give you the best results for your individual problem. The most common framework for this is most likely the [k-fold cross validation](http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#K-fold_cross-validation). However, even for a testing procedure, we need to choose some (k) numbers of nodes.\n",
    "The following  [formula](https://stats.stackexchange.com/a/136542) can give you a starting point:\n",
    "$$N_h = \\frac{N_s} {(\\alpha * (N_i + N_o))}$$\n",
    "$N_i$ is the number of input neurons, $N_o$ the number of output neurons, $N_s$ the number of samples in the trainings data, and $\\alpha$ represents a scaling factor that is usually between 2 and 10. We can calculate 8 different numbers to feed into our valdiation procedure and find the optimal model, based on the resulting validation loss. \n",
    "\n",
    "If the problem is simple and time an issue, there are various other rules of thumbs to determine the number of nodes, which are mostly simply based on the input and output neurons. We have to keep in mind that, while easy to use, they will rarely yield the optimal result. Here is just one example, which we will use for this basic model:\n",
    "$$N_h = \\frac{2} {3} *(N_i + N_o)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of hidden nodes is 460.\n"
     ]
    }
   ],
   "source": [
    "hidden_nodes = int(2/3 * (word_vec_length * char_vec_length))\n",
    "print(f\"The number of hidden nodes is {hidden_nodes}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the same uncertainty about the amount also exists for the number of hidden layers to use. Again, the ideal number for any given use case will be different and is best to be decided by running different models against each other. Generally, 2 layers have shown to be enough to detect more complex features. More layers can be better but also harder to train. As a general rule of thumb - 1 hidden layer work with simple problems, like this, and two are enough to find reasonably complex features. <br>\n",
    "In our case, adding a second layer only improves the accuracy by ~0.2% (0.9807 vs. 0.9819) after 10 epochs.\n",
    "\n",
    "### Choosing additional Hyper-Parameters\n",
    "\n",
    "Every LSTM layer should be accompanied by a Dropout layer. This layer will help to prevent overfitting by ignoring randomly selected neurons during training, and hence reduces the sensitivity to the specific weights of individual neurons. 20% is often used as a good compromise between retaining model accuracy and preventing overfitting.\n",
    "\n",
    "After our LSTM layer(s) did all the work to transform the input to make predictions towards the desired output possible, we have to reduce (or, in rare cases extend) the shape to match our desired output. In our case, we have two output labels and therefore we need two-output units. \n",
    "\n",
    "The final layer to add is the activation layer. Technically, this can be included into the density layer, but there is a reason to split this apart. While not relevant here, splitting the density layer and the activation layer makes it possible to retrieve the reduced output of the density layer of the model. Which activation function to use is, again, depending on the application. For our problem at hand, we have multiple classes (male and female) but only one of the classes can be present at a time. For these types of problems, generally, the [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function) works best, because it allows us (and your model) to interpret the outputs as probabilities.\n",
    "\n",
    "Loss function and activation function are often chosen together. Using the softmax activation function points us to cross-entropy as our preferred loss function or more precise the binary cross-entropy, since we are faced with a binary classification problem. Those two functions work well with each other because the cross-entropy function cancels out the plateaus at each end of the soft-max function and therefore speeds up the learning process.\n",
    "\n",
    "For choosing the optimizer, adaptive moment estimation, short _Adam_, has been shown to work well in most practical applications and works well with only little changes in the hyperparameters. Last but not least we have to decide, after which metric we want to judge our model. Keras offered [multiple accuracy functions](https://keras.io/metrics/). In many cases, judging the models' performance from an overall _accuracy_ point of view will be the option easiest to interpret as well as sufficient in resulting model performance.\n",
    "\n",
    "## Building, training and evaluating the model\n",
    "After getting some intuition about how to chose the most important parameters, let's put them all together and train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=output_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 299779 samples, validate on 99926 samples\n",
      "Epoch 1/10\n",
      "299779/299779 [==============================] - 40s 135us/step - loss: 0.3685 - acc: 0.8366 - val_loss: 0.1959 - val_acc: 0.9312\n",
      "Epoch 2/10\n",
      "299779/299779 [==============================] - 38s 127us/step - loss: 0.1538 - acc: 0.9489 - val_loss: 0.1149 - val_acc: 0.9643\n",
      "Epoch 3/10\n",
      "299779/299779 [==============================] - 38s 128us/step - loss: 0.1065 - acc: 0.9674 - val_loss: 0.0920 - val_acc: 0.9725\n",
      "Epoch 4/10\n",
      "299779/299779 [==============================] - 38s 127us/step - loss: 0.0932 - acc: 0.9722 - val_loss: 0.0866 - val_acc: 0.9739\n",
      "Epoch 5/10\n",
      "299779/299779 [==============================] - 38s 127us/step - loss: 0.0819 - acc: 0.9753 - val_loss: 0.0777 - val_acc: 0.9761\n",
      "Epoch 6/10\n",
      "299779/299779 [==============================] - 38s 127us/step - loss: 0.0752 - acc: 0.9770 - val_loss: 0.0744 - val_acc: 0.9770\n",
      "Epoch 7/10\n",
      "299779/299779 [==============================] - 38s 127us/step - loss: 0.0698 - acc: 0.9787 - val_loss: 0.0704 - val_acc: 0.9791\n",
      "Epoch 8/10\n",
      "299779/299779 [==============================] - 38s 127us/step - loss: 0.0648 - acc: 0.9800 - val_loss: 0.0657 - val_acc: 0.9795\n",
      "Epoch 9/10\n",
      "299779/299779 [==============================] - 38s 127us/step - loss: 0.0611 - acc: 0.9806 - val_loss: 0.0584 - val_acc: 0.9824\n",
      "Epoch 10/10\n",
      "299779/299779 [==============================] - 38s 127us/step - loss: 0.0567 - acc: 0.9820 - val_loss: 0.0567 - val_acc: 0.9821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29a023ffb70>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=1000\n",
    "model.fit(train_x, train_y, batch_size=batch_size, epochs=10, validation_data=(validate_x, validate_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 98.2% is pretty impressive and will most likely result from the fact that most names in the validation set were already present in our test set. Using our validation set we can take a quick look on where our model went wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>first_name</th>\n",
       "      <th>first_firstname</th>\n",
       "      <th>predicted_gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295484</th>\n",
       "      <td>f</td>\n",
       "      <td>Tordis</td>\n",
       "      <td>Tordis</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164216</th>\n",
       "      <td>m</td>\n",
       "      <td>Kaufman</td>\n",
       "      <td>Kaufman</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355847</th>\n",
       "      <td>m</td>\n",
       "      <td>Veli</td>\n",
       "      <td>Veli</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149883</th>\n",
       "      <td>f</td>\n",
       "      <td>Seher</td>\n",
       "      <td>Seher</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178947</th>\n",
       "      <td>f</td>\n",
       "      <td>Spohn</td>\n",
       "      <td>Spohn</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       gender first_name first_firstname predicted_gender\n",
       "295484      f     Tordis          Tordis                m\n",
       "164216      m    Kaufman         Kaufman                f\n",
       "355847      m       Veli            Veli                f\n",
       "149883      f      Seher           Seher                m\n",
       "178947      f      Spohn           Spohn                m"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate['predicted_gender'] = ['m' if prediction[0] > prediction[1] else 'f' for prediction in model.predict(validate_x)]\n",
    "validate[validate['gender'] != validate['predicted_gender']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, at least some of the false predictions seem to occur for people that typed in their family name into the first name field. Seeing that, a good next step seems to clean the original dataset from those cases. For now, the result looks pretty promising. With the accuracy we can archive, this model could already be used in many real-world situations. Also, training the model for more epochs might increase its performance, important here is to looks out for the performance on the validation set to prevent a possible overfitting\n",
    "\n",
    "## Conclusion\n",
    "In this article, we have successfully build a small model to predict the gender from a given (German) first name with an over 98% accuracy rate. While Keras frees us from writing complex deep learning algorithms, we still have to make choices regarding some of the hyperparameters along the way. In some cases, e.g. choosing the right activation function, we can rely on rules of thumbs or can determine the right parameter based on our problem. However, in some other cases, the best result will come from testing various configurations and then evaluating the outcome."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
